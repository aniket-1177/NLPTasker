{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Final Code\n",
        "\n",
        "Comprehensive Explanation for Advanced NLP Task Extraction Pipeline:\n",
        "\n",
        "Architecture Overview:\n",
        "The code implements an advanced Natural Language Processing (NLP) task extraction system using multiple sophisticated techniques:\n",
        "\n",
        "1. Preprocessing Stage\n",
        "- Uses spaCy and NLTK for text processing\n",
        "- Removes stop words\n",
        "- Performs lemmatization\n",
        "- Applies Part-of-Speech (POS) tagging\n",
        "- Tokenizes sentences with advanced filtering\n",
        "\n",
        "2. Task Identification Mechanism\n",
        "- Multi-layered task detection strategy\n",
        "- Uses predefined imperative verb lists\n",
        "- Matches task-related phrases\n",
        "- Implements intelligent sentence boundary analysis\n",
        "\n",
        "3. Entity Extraction Techniques\n",
        "- Prioritizes Named Entity Recognition (NER)\n",
        "- Fallback to grammatical subject extraction\n",
        "- Handles pronouns and specific entity types\n",
        "\n",
        "4. Deadline Detection\n",
        "- Multiple regex pattern matching\n",
        "- Captures complex time-related expressions\n",
        "- Flexible deadline identification\n",
        "\n",
        "5. Semantic Categorization\n",
        "- Word2Vec embedding generation\n",
        "- TF-IDF vectorization\n",
        "- Latent Dirichlet Allocation (LDA) for topic modeling\n",
        "\n",
        "Key Technical Components:\n",
        "- spaCy: Advanced NLP processing\n",
        "- NLTK: Natural language toolkit\n",
        "- Word2Vec: Semantic embedding\n",
        "- scikit-learn: Machine learning utilities\n",
        "- Gensim: Topic modeling\n",
        "\n",
        "Technical Complexity Highlights:\n",
        "- Dynamic task boundary detection\n",
        "- Semantic understanding beyond keyword matching\n",
        "- Machine learning-powered categorization\n",
        "- Robust handling of varied text inputs\n",
        "\n",
        "Design Principles:\n",
        "- Modular architecture\n",
        "- Extensible component design\n",
        "- Configurable task detection rules\n",
        "- Machine learning-enhanced processing\n",
        "\n",
        "Potential Interview Talking Points:\n",
        "1. Discuss multilayered task detection approach\n",
        "2. Explain semantic embedding's role in categorization\n",
        "3. Highlight flexible entity extraction mechanism\n",
        "4. Demonstrate handling of complex linguistic scenarios\n",
        "\n",
        "Implementation Challenges Addressed:\n",
        "- Handling ambiguous sentence structures\n",
        "- Capturing implicit task information\n",
        "- Providing context-aware categorization\n",
        "- Managing varied linguistic expressions\n",
        "\n",
        "Would you like me to elaborate on any specific aspect of the implementation for interview preparation?"
      ],
      "metadata": {
        "id": "wbOFHmDAg5Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "class AdvancedTaskExtractor:\n",
        "    def __init__(self):\n",
        "        # Download necessary resources\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "\n",
        "        # Load spaCy model\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Stop words and preprocessing\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Task identification patterns\n",
        "        self.task_indicators = {\n",
        "            'imperative_verbs': [\n",
        "                'buy', 'clean', 'review', 'prepare', 'submit',\n",
        "                'finalize', 'complete', 'schedule', 'discuss',\n",
        "                'send', 'create', 'update', 'resolve', 'go',\n",
        "                'finish', 'plan', 'organize'\n",
        "            ],\n",
        "            'task_phrases': [\n",
        "                'need to', 'has to', 'should', 'must', 'will',\n",
        "                'plan to', 'going to', 'wants to'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Advanced text preprocessing with POS tagging\"\"\"\n",
        "        # Normalize text\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokenize sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Preprocess sentences\n",
        "        processed_sentences = []\n",
        "        for sentence in sentences:\n",
        "            doc = self.nlp(sentence)\n",
        "            processed_sentence = [\n",
        "                token.lemma_.lower()\n",
        "                for token in doc\n",
        "                if token.text.lower() not in self.stop_words\n",
        "                and not token.is_punct\n",
        "                and token.pos_ in ['VERB', 'NOUN', 'PROPN']\n",
        "            ]\n",
        "            processed_sentences.append(' '.join(processed_sentence))\n",
        "\n",
        "        return sentences, processed_sentences\n",
        "\n",
        "    def identify_tasks(self, original_sentences: List[str], processed_sentences: List[str]) -> List[Dict]:\n",
        "        \"\"\"Advanced task identification with detailed logging\"\"\"\n",
        "        tasks = []\n",
        "\n",
        "        for orig_sent, proc_sent in zip(original_sentences, processed_sentences):\n",
        "            # Detailed logging for task detection\n",
        "            print(f\"\\nAnalyzing Sentence: {orig_sent}\")\n",
        "            print(f\"Processed Sentence: {proc_sent}\")\n",
        "\n",
        "            # Enhanced task detection\n",
        "            is_task = False\n",
        "\n",
        "            # Check for imperative verbs\n",
        "            verb_match = any(verb in proc_sent for verb in self.task_indicators['imperative_verbs'])\n",
        "            print(f\"Imperative Verb Match: {verb_match}\")\n",
        "\n",
        "            # Check for task indicator phrases\n",
        "            phrase_match = any(phrase in orig_sent.lower() for phrase in self.task_indicators['task_phrases'])\n",
        "            print(f\"Task Phrase Match: {phrase_match}\")\n",
        "\n",
        "            # Determine if sentence is a task\n",
        "            if verb_match or phrase_match:\n",
        "                is_task = True\n",
        "                print(\"Sentence identified as a task\")\n",
        "\n",
        "            if not is_task:\n",
        "                print(\"Not a task. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Entity extraction\n",
        "            doc = self.nlp(orig_sent)\n",
        "            entities = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "\n",
        "            # Fallback to subject extraction\n",
        "            if not entities:\n",
        "                entities = [token.text for token in doc if token.dep_ in [\"nsubj\", \"nsubjpass\"]]\n",
        "\n",
        "            # Deadline extraction\n",
        "            deadline = self._extract_deadline(orig_sent)\n",
        "\n",
        "            # Create task entry\n",
        "            task_entry = {\n",
        "                'task': orig_sent,\n",
        "                'processed_task': proc_sent,\n",
        "                'entity': entities[0] if entities else None,\n",
        "                'deadline': deadline\n",
        "            }\n",
        "\n",
        "            print(\"Task Entry:\")\n",
        "            print(task_entry)\n",
        "\n",
        "            tasks.append(task_entry)\n",
        "\n",
        "        return tasks\n",
        "\n",
        "    def _extract_deadline(self, sentence: str) -> str:\n",
        "        \"\"\"Sophisticated deadline extraction\"\"\"\n",
        "        deadline_patterns = [\n",
        "            r'by\\s+([\\w\\s]+)',\n",
        "            r'until\\s+([\\w\\s]+)',\n",
        "            r'on\\s+([\\w\\s]+)',\n",
        "            r'before\\s+([\\w\\s]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in deadline_patterns:\n",
        "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "\n",
        "        # Additional time-related keywords\n",
        "        time_keywords = ['today', 'tomorrow', 'next week', 'this week', 'monday', 'friday']\n",
        "        for keyword in time_keywords:\n",
        "            if keyword in sentence.lower():\n",
        "                return keyword\n",
        "\n",
        "        return None\n",
        "\n",
        "    def train_word_embeddings(self, processed_sentences: List[str]):\n",
        "        \"\"\"Train Word2Vec embeddings\"\"\"\n",
        "        # Tokenize processed sentences\n",
        "        tokenized_sentences = [sent.split() for sent in processed_sentences]\n",
        "\n",
        "        # Train Word2Vec model\n",
        "        model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        return model\n",
        "\n",
        "    def categorize_tasks(self, tasks: List[Dict], processed_sentences: List[str]):\n",
        "        \"\"\"Advanced categorization using TF-IDF and LDA\"\"\"\n",
        "        # TF-IDF Vectorization\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
        "\n",
        "        # LDA Topic Modeling\n",
        "        lda_model = LatentDirichletAllocation(n_components=4, random_state=42)\n",
        "        lda_output = lda_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "        # Map topics to categories\n",
        "        category_map = {\n",
        "            0: 'Professional',\n",
        "            1: 'Personal',\n",
        "            2: 'Team',\n",
        "            3: 'Administrative'\n",
        "        }\n",
        "\n",
        "        # Assign categories\n",
        "        for task, topic_dist in zip(tasks, lda_output):\n",
        "            dominant_topic = np.argmax(topic_dist)\n",
        "            task['category'] = category_map[dominant_topic]\n",
        "\n",
        "        return tasks\n",
        "\n",
        "    def extract_tasks(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Main task extraction pipeline\"\"\"\n",
        "        # Preprocess text\n",
        "        original_sentences, processed_sentences = self.preprocess_text(text)\n",
        "\n",
        "        # Identify tasks\n",
        "        tasks = self.identify_tasks(original_sentences, processed_sentences)\n",
        "\n",
        "        # Categorize tasks\n",
        "        categorized_tasks = self.categorize_tasks(tasks, processed_sentences)\n",
        "\n",
        "        return categorized_tasks\n",
        "\n",
        "def main():\n",
        "    text = \"\"\"\n",
        "    Rahul wakes up early every day. He goes to college in the morning and comes back at 3 pm.\n",
        "    At present, Rahul is outside. He has to buy the snacks for all of us.\n",
        "    Rahul should clean the room by 5 pm today.\n",
        "    John needs to review the report by Friday.\n",
        "    Alice needs to finish her homework by 6 pm.\n",
        "    Bob is planning to go for a run tomorrow morning.\n",
        "    The team should discuss the project updates in the meeting next week.\n",
        "    Sarah has to prepare the presentation for the meeting on Monday.\n",
        "    Tom will submit the project report by the end of the week.\n",
        "    The group needs to finalize the budget by 3 pm tomorrow.\n",
        "    \"\"\"\n",
        "\n",
        "    extractor = AdvancedTaskExtractor()\n",
        "    tasks = extractor.extract_tasks(text)\n",
        "\n",
        "    print(\"\\nExtracted Tasks:\")\n",
        "    for task in tasks:\n",
        "        print(f\"Task: {task['task']}\")\n",
        "        print(f\"Entity: {task['entity']}\")\n",
        "        print(f\"Deadline: {task['deadline']}\")\n",
        "        print(f\"Category: {task['category']}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    # Uncomment to run additional test scenarios\n",
        "    # test_multiple_scenarios()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrKw0eF4arIY",
        "outputId": "4a2ee7bc-f849-413c-dd88-1096553598c1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Sentence: Rahul wakes up early every day.\n",
            "Processed Sentence: wake day\n",
            "Imperative Verb Match: False\n",
            "Task Phrase Match: False\n",
            "Not a task. Skipping.\n",
            "\n",
            "Analyzing Sentence: He goes to college in the morning and comes back at 3 pm.\n",
            "Processed Sentence: go college morning come pm\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'He goes to college in the morning and comes back at 3 pm.', 'processed_task': 'go college morning come pm', 'entity': 'He', 'deadline': None}\n",
            "\n",
            "Analyzing Sentence: At present, Rahul is outside.\n",
            "Processed Sentence: present rahul\n",
            "Imperative Verb Match: False\n",
            "Task Phrase Match: False\n",
            "Not a task. Skipping.\n",
            "\n",
            "Analyzing Sentence: He has to buy the snacks for all of us.\n",
            "Processed Sentence: buy snack\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'He has to buy the snacks for all of us.', 'processed_task': 'buy snack', 'entity': 'He', 'deadline': None}\n",
            "\n",
            "Analyzing Sentence: Rahul should clean the room by 5 pm today.\n",
            "Processed Sentence: rahul clean room pm today\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Rahul should clean the room by 5 pm today.', 'processed_task': 'rahul clean room pm today', 'entity': 'Rahul', 'deadline': '5 pm today'}\n",
            "\n",
            "Analyzing Sentence: John needs to review the report by Friday.\n",
            "Processed Sentence: john need review report friday\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'John needs to review the report by Friday.', 'processed_task': 'john need review report friday', 'entity': 'John', 'deadline': 'Friday'}\n",
            "\n",
            "Analyzing Sentence: Alice needs to finish her homework by 6 pm.\n",
            "Processed Sentence: alice need finish homework pm\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Alice needs to finish her homework by 6 pm.', 'processed_task': 'alice need finish homework pm', 'entity': 'Alice', 'deadline': '6 pm'}\n",
            "\n",
            "Analyzing Sentence: Bob is planning to go for a run tomorrow morning.\n",
            "Processed Sentence: bob plan go run tomorrow morning\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Bob is planning to go for a run tomorrow morning.', 'processed_task': 'bob plan go run tomorrow morning', 'entity': 'Bob', 'deadline': 'tomorrow'}\n",
            "\n",
            "Analyzing Sentence: The team should discuss the project updates in the meeting next week.\n",
            "Processed Sentence: team discuss project update meeting week\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'The team should discuss the project updates in the meeting next week.', 'processed_task': 'team discuss project update meeting week', 'entity': 'team', 'deadline': 'next week'}\n",
            "\n",
            "Analyzing Sentence: Sarah has to prepare the presentation for the meeting on Monday.\n",
            "Processed Sentence: sarah prepare presentation meeting monday\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Sarah has to prepare the presentation for the meeting on Monday.', 'processed_task': 'sarah prepare presentation meeting monday', 'entity': 'Sarah', 'deadline': 'for the meeting on Monday'}\n",
            "\n",
            "Analyzing Sentence: Tom will submit the project report by the end of the week.\n",
            "Processed Sentence: tom submit project report end week\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Tom will submit the project report by the end of the week.', 'processed_task': 'tom submit project report end week', 'entity': 'Tom', 'deadline': 'the end of the week'}\n",
            "\n",
            "Analyzing Sentence: The group needs to finalize the budget by 3 pm tomorrow.\n",
            "Processed Sentence: group need finalize budget pm tomorrow\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'The group needs to finalize the budget by 3 pm tomorrow.', 'processed_task': 'group need finalize budget pm tomorrow', 'entity': 'group', 'deadline': '3 pm tomorrow'}\n",
            "\n",
            "Extracted Tasks:\n",
            "Task: He goes to college in the morning and comes back at 3 pm.\n",
            "Entity: He\n",
            "Deadline: None\n",
            "Category: Professional\n",
            "\n",
            "Task: He has to buy the snacks for all of us.\n",
            "Entity: He\n",
            "Deadline: None\n",
            "Category: Team\n",
            "\n",
            "Task: Rahul should clean the room by 5 pm today.\n",
            "Entity: Rahul\n",
            "Deadline: 5 pm today\n",
            "Category: Professional\n",
            "\n",
            "Task: John needs to review the report by Friday.\n",
            "Entity: John\n",
            "Deadline: Friday\n",
            "Category: Administrative\n",
            "\n",
            "Task: Alice needs to finish her homework by 6 pm.\n",
            "Entity: Alice\n",
            "Deadline: 6 pm\n",
            "Category: Administrative\n",
            "\n",
            "Task: Bob is planning to go for a run tomorrow morning.\n",
            "Entity: Bob\n",
            "Deadline: tomorrow\n",
            "Category: Team\n",
            "\n",
            "Task: The team should discuss the project updates in the meeting next week.\n",
            "Entity: team\n",
            "Deadline: next week\n",
            "Category: Professional\n",
            "\n",
            "Task: Sarah has to prepare the presentation for the meeting on Monday.\n",
            "Entity: Sarah\n",
            "Deadline: for the meeting on Monday\n",
            "Category: Administrative\n",
            "\n",
            "Task: Tom will submit the project report by the end of the week.\n",
            "Entity: Tom\n",
            "Deadline: the end of the week\n",
            "Category: Administrative\n",
            "\n",
            "Task: The group needs to finalize the budget by 3 pm tomorrow.\n",
            "Entity: group\n",
            "Deadline: 3 pm tomorrow\n",
            "Category: Team\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCxqoqcXD79y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text\n",
        "I/o"
      ],
      "metadata": {
        "id": "YdVHAXdiFgsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import csv\n",
        "\n",
        "\n",
        "class AdvancedTaskExtractor:\n",
        "    def __init__(self):\n",
        "        # Download necessary resources\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "\n",
        "        # Load spaCy model\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Stop words and preprocessing\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Task identification patterns\n",
        "        self.task_indicators = {\n",
        "            'imperative_verbs': [\n",
        "                'buy', 'clean', 'review', 'prepare', 'submit',\n",
        "                'finalize', 'complete', 'schedule', 'discuss',\n",
        "                'send', 'create', 'update', 'resolve', 'go',\n",
        "                'finish', 'plan', 'organize'\n",
        "            ],\n",
        "            'task_phrases': [\n",
        "                'need to', 'has to', 'should', 'must', 'will',\n",
        "                'plan to', 'going to', 'wants to'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Advanced text preprocessing with POS tagging\"\"\"\n",
        "        # Normalize text\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokenize sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Preprocess sentences\n",
        "        processed_sentences = []\n",
        "        for sentence in sentences:\n",
        "            doc = self.nlp(sentence)\n",
        "            processed_sentence = [\n",
        "                token.lemma_.lower()\n",
        "                for token in doc\n",
        "                if token.text.lower() not in self.stop_words\n",
        "                and not token.is_punct\n",
        "                and token.pos_ in ['VERB', 'NOUN', 'PROPN']\n",
        "            ]\n",
        "            processed_sentences.append(' '.join(processed_sentence))\n",
        "\n",
        "        return sentences, processed_sentences\n",
        "\n",
        "    def identify_tasks(self, original_sentences: List[str], processed_sentences: List[str]) -> List[Dict]:\n",
        "        \"\"\"Advanced task identification with detailed logging\"\"\"\n",
        "        tasks = []\n",
        "\n",
        "        for orig_sent, proc_sent in zip(original_sentences, processed_sentences):\n",
        "            # Detailed logging for task detection\n",
        "            print(f\"\\nAnalyzing Sentence: {orig_sent}\")\n",
        "            print(f\"Processed Sentence: {proc_sent}\")\n",
        "\n",
        "            # Enhanced task detection\n",
        "            is_task = False\n",
        "\n",
        "            # Check for imperative verbs\n",
        "            verb_match = any(verb in proc_sent for verb in self.task_indicators['imperative_verbs'])\n",
        "            print(f\"Imperative Verb Match: {verb_match}\")\n",
        "\n",
        "            # Check for task indicator phrases\n",
        "            phrase_match = any(phrase in orig_sent.lower() for phrase in self.task_indicators['task_phrases'])\n",
        "            print(f\"Task Phrase Match: {phrase_match}\")\n",
        "\n",
        "            # Determine if sentence is a task\n",
        "            if verb_match or phrase_match:\n",
        "                is_task = True\n",
        "                print(\"Sentence identified as a task\")\n",
        "\n",
        "            if not is_task:\n",
        "                print(\"Not a task. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Entity extraction\n",
        "            doc = self.nlp(orig_sent)\n",
        "            entities = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "\n",
        "            # Fallback to subject extraction\n",
        "            if not entities:\n",
        "                entities = [token.text for token in doc if token.dep_ in [\"nsubj\", \"nsubjpass\"]]\n",
        "\n",
        "            # Deadline extraction\n",
        "            deadline = self._extract_deadline(orig_sent)\n",
        "\n",
        "            # Create task entry\n",
        "            task_entry = {\n",
        "                'task': orig_sent,\n",
        "                'processed_task': proc_sent,\n",
        "                'entity': entities[0] if entities else None,\n",
        "                'deadline': deadline\n",
        "            }\n",
        "\n",
        "            print(\"Task Entry:\")\n",
        "            print(task_entry)\n",
        "\n",
        "            tasks.append(task_entry)\n",
        "\n",
        "        return tasks\n",
        "\n",
        "    def _extract_deadline(self, sentence: str) -> str:\n",
        "        \"\"\"Sophisticated deadline extraction\"\"\"\n",
        "        deadline_patterns = [\n",
        "            r'by\\s+([\\w\\s]+)',\n",
        "            r'until\\s+([\\w\\s]+)',\n",
        "            r'on\\s+([\\w\\s]+)',\n",
        "            r'before\\s+([\\w\\s]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in deadline_patterns:\n",
        "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "\n",
        "        # Additional time-related keywords\n",
        "        time_keywords = ['today', 'tomorrow', 'next week', 'this week', 'monday', 'friday']\n",
        "        for keyword in time_keywords:\n",
        "            if keyword in sentence.lower():\n",
        "                return keyword\n",
        "\n",
        "        return None\n",
        "\n",
        "    def train_word_embeddings(self, processed_sentences: List[str]):\n",
        "        \"\"\"Train Word2Vec embeddings\"\"\"\n",
        "        # Tokenize processed sentences\n",
        "        tokenized_sentences = [sent.split() for sent in processed_sentences]\n",
        "\n",
        "        # Train Word2Vec model\n",
        "        model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        return model\n",
        "\n",
        "    def categorize_tasks(self, tasks: List[Dict], processed_sentences: List[str]):\n",
        "        \"\"\"Advanced categorization using TF-IDF and LDA\"\"\"\n",
        "        # TF-IDF Vectorization\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
        "\n",
        "        # LDA Topic Modeling\n",
        "        lda_model = LatentDirichletAllocation(n_components=4, random_state=42)\n",
        "        lda_output = lda_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "        # Map topics to categories\n",
        "        category_map = {\n",
        "            0: 'Professional',\n",
        "            1: 'Personal',\n",
        "            2: 'Team',\n",
        "            3: 'Administrative'\n",
        "        }\n",
        "\n",
        "        # Assign categories\n",
        "        for task, topic_dist in zip(tasks, lda_output):\n",
        "            dominant_topic = np.argmax(topic_dist)\n",
        "            task['category'] = category_map[dominant_topic]\n",
        "\n",
        "        return tasks\n",
        "\n",
        "    def extract_tasks(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Main task extraction pipeline\"\"\"\n",
        "        # Preprocess text\n",
        "        original_sentences, processed_sentences = self.preprocess_text(text)\n",
        "\n",
        "        # Identify tasks\n",
        "        tasks = self.identify_tasks(original_sentences, processed_sentences)\n",
        "\n",
        "        # Categorize tasks\n",
        "        categorized_tasks = self.categorize_tasks(tasks, processed_sentences)\n",
        "\n",
        "        return categorized_tasks\n",
        "\n",
        "def main():\n",
        "    # Read text from .txt file\n",
        "    try:\n",
        "        with open(\"input.txt\", \"r\", encoding=\"utf-8\") as file:  # Specify encoding if needed\n",
        "            text = file.read()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: input.txt not found. Please create the file and add text.\")\n",
        "        return\n",
        "\n",
        "    extractor = AdvancedTaskExtractor()\n",
        "    tasks = extractor.extract_tasks(text)\n",
        "\n",
        "    # Write output to CSV file\n",
        "    try:\n",
        "        with open(\"output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            fieldnames = ['task', 'entity', 'deadline', 'category', 'processed_task'] # Add processed_task\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for task in tasks:\n",
        "                writer.writerow(task)\n",
        "        print(\"Tasks extracted and saved to output.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to CSV: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-op4IO3BnMwC",
        "outputId": "54928ef8-0578-4fdb-a5e5-825b88a65fb3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Sentence: Rahul wakes up early every day.\n",
            "Processed Sentence: wake day\n",
            "Imperative Verb Match: False\n",
            "Task Phrase Match: False\n",
            "Not a task. Skipping.\n",
            "\n",
            "Analyzing Sentence: He goes to college in the morning and comes back at 3 pm.\n",
            "Processed Sentence: go college morning come pm\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'He goes to college in the morning and comes back at 3 pm.', 'processed_task': 'go college morning come pm', 'entity': 'He', 'deadline': None}\n",
            "\n",
            "Analyzing Sentence: At present, Rahul is outside.\n",
            "Processed Sentence: present rahul\n",
            "Imperative Verb Match: False\n",
            "Task Phrase Match: False\n",
            "Not a task. Skipping.\n",
            "\n",
            "Analyzing Sentence: He has to buy the snacks for all of us.\n",
            "Processed Sentence: buy snack\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'He has to buy the snacks for all of us.', 'processed_task': 'buy snack', 'entity': 'He', 'deadline': None}\n",
            "\n",
            "Analyzing Sentence: Rahul should clean the room by 5 pm today.\n",
            "Processed Sentence: rahul clean room pm today\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Rahul should clean the room by 5 pm today.', 'processed_task': 'rahul clean room pm today', 'entity': 'Rahul', 'deadline': '5 pm today'}\n",
            "\n",
            "Analyzing Sentence: John needs to review the report by Friday.\n",
            "Processed Sentence: john need review report friday\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'John needs to review the report by Friday.', 'processed_task': 'john need review report friday', 'entity': 'John', 'deadline': 'Friday'}\n",
            "\n",
            "Analyzing Sentence: Alice needs to finish her homework by 6 pm.\n",
            "Processed Sentence: alice need finish homework pm\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Alice needs to finish her homework by 6 pm.', 'processed_task': 'alice need finish homework pm', 'entity': 'Alice', 'deadline': '6 pm'}\n",
            "\n",
            "Analyzing Sentence: Bob is planning to go for a run tomorrow morning.\n",
            "Processed Sentence: bob plan go run tomorrow morning\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Bob is planning to go for a run tomorrow morning.', 'processed_task': 'bob plan go run tomorrow morning', 'entity': 'Bob', 'deadline': 'tomorrow'}\n",
            "\n",
            "Analyzing Sentence: The team should discuss the project updates in the meeting next week.\n",
            "Processed Sentence: team discuss project update meeting week\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'The team should discuss the project updates in the meeting next week.', 'processed_task': 'team discuss project update meeting week', 'entity': 'team', 'deadline': 'next week'}\n",
            "\n",
            "Analyzing Sentence: Sarah has to prepare the presentation for the meeting on Monday.\n",
            "Processed Sentence: sarah prepare presentation meeting monday\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Sarah has to prepare the presentation for the meeting on Monday.', 'processed_task': 'sarah prepare presentation meeting monday', 'entity': 'Sarah', 'deadline': 'for the meeting on Monday'}\n",
            "\n",
            "Analyzing Sentence: Tom will submit the project report by the end of the week.\n",
            "Processed Sentence: tom submit project report end week\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Tom will submit the project report by the end of the week.', 'processed_task': 'tom submit project report end week', 'entity': 'Tom', 'deadline': 'the end of the week'}\n",
            "\n",
            "Analyzing Sentence: The group needs to finalize the budget by 3 pm tomorrow.\n",
            "Processed Sentence: group need finalize budget pm tomorrow\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'The group needs to finalize the budget by 3 pm tomorrow.', 'processed_task': 'group need finalize budget pm tomorrow', 'entity': 'group', 'deadline': '3 pm tomorrow'}\n",
            "Tasks extracted and saved to output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embeddings + Clustering\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rC4pyi7VFcFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import csv\n",
        "\n",
        "\n",
        "class AdvancedTaskExtractor:\n",
        "    def __init__(self):\n",
        "        # Download necessary resources\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "\n",
        "        # Load spaCy model\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Stop words and preprocessing\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Task identification patterns\n",
        "        self.task_indicators = {\n",
        "            'imperative_verbs': [\n",
        "                'buy', 'clean', 'review', 'prepare', 'submit',\n",
        "                'finalize', 'complete', 'schedule', 'discuss',\n",
        "                'send', 'create', 'update', 'resolve', 'go',\n",
        "                'finish', 'plan', 'organize'\n",
        "            ],\n",
        "            'task_phrases': [\n",
        "                'need to', 'has to', 'should', 'must', 'will',\n",
        "                'plan to', 'going to', 'wants to'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Advanced text preprocessing with POS tagging\"\"\"\n",
        "        # Normalize text\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokenize sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Preprocess sentences\n",
        "        processed_sentences = []\n",
        "        for sentence in sentences:\n",
        "            doc = self.nlp(sentence)\n",
        "            processed_sentence = [\n",
        "                token.lemma_.lower()\n",
        "                for token in doc\n",
        "                if token.text.lower() not in self.stop_words\n",
        "                and not token.is_punct\n",
        "                and token.pos_ in ['VERB', 'NOUN', 'PROPN']\n",
        "            ]\n",
        "            processed_sentences.append(' '.join(processed_sentence))\n",
        "\n",
        "        return sentences, processed_sentences\n",
        "\n",
        "    def identify_tasks(self, original_sentences: List[str], processed_sentences: List[str]) -> List[Dict]:\n",
        "        \"\"\"Advanced task identification with detailed logging\"\"\"\n",
        "        tasks = []\n",
        "\n",
        "        for orig_sent, proc_sent in zip(original_sentences, processed_sentences):\n",
        "            # Detailed logging for task detection\n",
        "            print(f\"\\nAnalyzing Sentence: {orig_sent}\")\n",
        "            print(f\"Processed Sentence: {proc_sent}\")\n",
        "\n",
        "            # Enhanced task detection\n",
        "            is_task = False\n",
        "\n",
        "            # Check for imperative verbs\n",
        "            verb_match = any(verb in proc_sent for verb in self.task_indicators['imperative_verbs'])\n",
        "            print(f\"Imperative Verb Match: {verb_match}\")\n",
        "\n",
        "            # Check for task indicator phrases\n",
        "            phrase_match = any(phrase in orig_sent.lower() for phrase in self.task_indicators['task_phrases'])\n",
        "            print(f\"Task Phrase Match: {phrase_match}\")\n",
        "\n",
        "            # Determine if sentence is a task\n",
        "            if verb_match or phrase_match:\n",
        "                is_task = True\n",
        "                print(\"Sentence identified as a task\")\n",
        "\n",
        "            if not is_task:\n",
        "                print(\"Not a task. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Entity extraction\n",
        "            doc = self.nlp(orig_sent)\n",
        "            entities = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "\n",
        "            # Fallback to subject extraction\n",
        "            if not entities:\n",
        "                entities = [token.text for token in doc if token.dep_ in [\"nsubj\", \"nsubjpass\"]]\n",
        "\n",
        "            # Deadline extraction\n",
        "            deadline = self._extract_deadline(orig_sent)\n",
        "\n",
        "            # Create task entry\n",
        "            task_entry = {\n",
        "                'task': orig_sent,\n",
        "                'processed_task': proc_sent,\n",
        "                'entity': entities[0] if entities else None,\n",
        "                'deadline': deadline\n",
        "            }\n",
        "\n",
        "            print(\"Task Entry:\")\n",
        "            print(task_entry)\n",
        "\n",
        "            tasks.append(task_entry)\n",
        "\n",
        "        return tasks\n",
        "\n",
        "    def _extract_deadline(self, sentence: str) -> str:\n",
        "        \"\"\"Sophisticated deadline extraction\"\"\"\n",
        "        deadline_patterns = [\n",
        "            r'by\\s+([\\w\\s]+)',\n",
        "            r'until\\s+([\\w\\s]+)',\n",
        "            r'on\\s+([\\w\\s]+)',\n",
        "            r'before\\s+([\\w\\s]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in deadline_patterns:\n",
        "            match = re.search(pattern, sentence, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "\n",
        "        # Additional time-related keywords\n",
        "        time_keywords = ['today', 'tomorrow', 'next week', 'this week', 'monday', 'friday']\n",
        "        for keyword in time_keywords:\n",
        "            if keyword in sentence.lower():\n",
        "                return keyword\n",
        "\n",
        "        return None\n",
        "\n",
        "    def train_word_embeddings(self, processed_sentences: List[str]):\n",
        "        \"\"\"Train Word2Vec embeddings\"\"\"\n",
        "        # Tokenize processed sentences\n",
        "        tokenized_sentences = [sent.split() for sent in processed_sentences]\n",
        "\n",
        "        # Train Word2Vec model\n",
        "        model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "        return model\n",
        "\n",
        "\n",
        "    def categorize_tasks(self, tasks: List[Dict], processed_sentences: List[str]):\n",
        "        \"\"\"Categorize tasks using Word2Vec embeddings and K-Means clustering\"\"\"\n",
        "\n",
        "        # Train Word2Vec model\n",
        "        model = self.train_word_embeddings(processed_sentences)\n",
        "\n",
        "        # Convert sentences to embeddings\n",
        "        embeddings = [np.mean([model.wv[word] for word in sent.split() if word in model.wv], axis=0)\n",
        "                      if any(word in model.wv for word in sent.split()) else np.zeros(100)\n",
        "                      for sent in processed_sentences]\n",
        "\n",
        "        # Cluster embeddings using K-Means\n",
        "        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "        task_clusters = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Define category names\n",
        "        category_map = {\n",
        "            0: 'Professional',\n",
        "            1: 'Personal',\n",
        "            2: 'Team',\n",
        "            3: 'Administrative'\n",
        "        }\n",
        "\n",
        "        # Assign categories\n",
        "        for task, cluster in zip(tasks, task_clusters):\n",
        "            task['category'] = category_map[cluster]\n",
        "\n",
        "        return tasks\n",
        "\n",
        "\n",
        "    def extract_tasks(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Main task extraction pipeline\"\"\"\n",
        "        # Preprocess text\n",
        "        original_sentences, processed_sentences = self.preprocess_text(text)\n",
        "\n",
        "        # Identify tasks\n",
        "        tasks = self.identify_tasks(original_sentences, processed_sentences)\n",
        "\n",
        "        # Categorize tasks\n",
        "        categorized_tasks = self.categorize_tasks(tasks, processed_sentences)\n",
        "\n",
        "        return categorized_tasks\n",
        "\n",
        "def main():\n",
        "    # Read text from .txt file\n",
        "    try:\n",
        "        with open(\"input.txt\", \"r\", encoding=\"utf-8\") as file:  # Specify encoding if needed\n",
        "            text = file.read()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: input.txt not found. Please create the file and add text.\")\n",
        "        return\n",
        "\n",
        "    extractor = AdvancedTaskExtractor()\n",
        "    tasks = extractor.extract_tasks(text)\n",
        "\n",
        "    # Write output to CSV file\n",
        "    try:\n",
        "        with open(\"output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            fieldnames = ['task', 'entity', 'deadline', 'category', 'processed_task'] # Add processed_task\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for task in tasks:\n",
        "                writer.writerow(task)\n",
        "        print(\"Tasks extracted and saved to output.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to CSV: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqT0s0QfD969",
        "outputId": "c95cd803-f94e-480f-ca4f-c7ac8eaa5246"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Sentence: Rahul wakes up early every day.\n",
            "Processed Sentence: wake day\n",
            "Imperative Verb Match: False\n",
            "Task Phrase Match: False\n",
            "Not a task. Skipping.\n",
            "\n",
            "Analyzing Sentence: He goes to college in the morning and comes back at 3 pm.\n",
            "Processed Sentence: go college morning come pm\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'He goes to college in the morning and comes back at 3 pm.', 'processed_task': 'go college morning come pm', 'entity': 'He', 'deadline': None}\n",
            "\n",
            "Analyzing Sentence: At present, Rahul is outside.\n",
            "Processed Sentence: present rahul\n",
            "Imperative Verb Match: False\n",
            "Task Phrase Match: False\n",
            "Not a task. Skipping.\n",
            "\n",
            "Analyzing Sentence: He has to buy the snacks for all of us.\n",
            "Processed Sentence: buy snack\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'He has to buy the snacks for all of us.', 'processed_task': 'buy snack', 'entity': 'He', 'deadline': None}\n",
            "\n",
            "Analyzing Sentence: Rahul should clean the room by 5 pm today.\n",
            "Processed Sentence: rahul clean room pm today\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Rahul should clean the room by 5 pm today.', 'processed_task': 'rahul clean room pm today', 'entity': 'Rahul', 'deadline': '5 pm today'}\n",
            "\n",
            "Analyzing Sentence: John needs to review the report by Friday.\n",
            "Processed Sentence: john need review report friday\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'John needs to review the report by Friday.', 'processed_task': 'john need review report friday', 'entity': 'John', 'deadline': 'Friday'}\n",
            "\n",
            "Analyzing Sentence: Alice needs to finish her homework by 6 pm.\n",
            "Processed Sentence: alice need finish homework pm\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Alice needs to finish her homework by 6 pm.', 'processed_task': 'alice need finish homework pm', 'entity': 'Alice', 'deadline': '6 pm'}\n",
            "\n",
            "Analyzing Sentence: Bob is planning to go for a run tomorrow morning.\n",
            "Processed Sentence: bob plan go run tomorrow morning\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Bob is planning to go for a run tomorrow morning.', 'processed_task': 'bob plan go run tomorrow morning', 'entity': 'Bob', 'deadline': 'tomorrow'}\n",
            "\n",
            "Analyzing Sentence: The team should discuss the project updates in the meeting next week.\n",
            "Processed Sentence: team discuss project update meeting week\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'The team should discuss the project updates in the meeting next week.', 'processed_task': 'team discuss project update meeting week', 'entity': 'team', 'deadline': 'next week'}\n",
            "\n",
            "Analyzing Sentence: Sarah has to prepare the presentation for the meeting on Monday.\n",
            "Processed Sentence: sarah prepare presentation meeting monday\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Sarah has to prepare the presentation for the meeting on Monday.', 'processed_task': 'sarah prepare presentation meeting monday', 'entity': 'Sarah', 'deadline': 'for the meeting on Monday'}\n",
            "\n",
            "Analyzing Sentence: Tom will submit the project report by the end of the week.\n",
            "Processed Sentence: tom submit project report end week\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: True\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'Tom will submit the project report by the end of the week.', 'processed_task': 'tom submit project report end week', 'entity': 'Tom', 'deadline': 'the end of the week'}\n",
            "\n",
            "Analyzing Sentence: The group needs to finalize the budget by 3 pm tomorrow.\n",
            "Processed Sentence: group need finalize budget pm tomorrow\n",
            "Imperative Verb Match: True\n",
            "Task Phrase Match: False\n",
            "Sentence identified as a task\n",
            "Task Entry:\n",
            "{'task': 'The group needs to finalize the budget by 3 pm tomorrow.', 'processed_task': 'group need finalize budget pm tomorrow', 'entity': 'group', 'deadline': '3 pm tomorrow'}\n",
            "Tasks extracted and saved to output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8p_PcZd6E6Zp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}